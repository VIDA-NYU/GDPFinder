{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import shapely\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from data import SmallPatchesDataset, get_filenames\n",
    "from models import AutoEncoder, DEC\n",
    "\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cities_used = 100\n",
    "\n",
    "# remove patches that are have small intersection with block\n",
    "intersection_threshold = 0.25\n",
    "\n",
    "# remove patches from blocks that have more than a threshold \n",
    "patches_count_max = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_df = gpd.read_file(\"../data/census_blocks_patches_v2.geojson\")\n",
    "\n",
    "# filtering to test with some blocks, selecting the cities with the biggest number of blocks\n",
    "biggest_blocks = blocks_df.groupby([\"state\", \"county\"]).agg(\"count\").reset_index().iloc[:, 0:3].sort_values(\"tract\", ascending = False).head(n_cities_used)\n",
    "blocks_df = blocks_df[(blocks_df.state.isin(biggest_blocks.state) & blocks_df.county.isin(biggest_blocks.county))]\n",
    "\n",
    "# cleaning blocks with missing data\n",
    "blocks_df = blocks_df[blocks_df.mhi > 0]\n",
    "blocks_df = blocks_df.dropna()\n",
    "blocks_df = blocks_df[blocks_df.patches_relation.apply(len) > 0]\n",
    "\n",
    "blocks_df[\"area_km2\"] = blocks_df['geometry'].to_crs({'proj':'cea'}).area / 10**6\n",
    "blocks_df[\"density\"] = blocks_df[\"pop\"] / blocks_df[\"area_km2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_patches_relation(s):\n",
    "    s = s.split(\"\\n\")\n",
    "    s = dict([x.split(\":\") for x in s])\n",
    "    filenames = []\n",
    "    data = []\n",
    "    for key, value in s.items():\n",
    "        value = value.split(\" \")\n",
    "        idx = np.array([float(v) for v in value[0].split(\",\")])\n",
    "        ratio = np.array([float(v) for v in value[1].split(\",\")])\n",
    "        idx = idx[ratio > intersection_threshold]\n",
    "        ratio = ratio[ratio > intersection_threshold]\n",
    "        for i in range(len(idx)):\n",
    "            data.append([idx[i], ratio[i]])\n",
    "            filenames.append(key)\n",
    "    data = np.array(data)\n",
    "    if len(filenames) > patches_count_max:\n",
    "        selected = np.random.choice(\n",
    "            len(filenames),\n",
    "            size=patches_count_max,\n",
    "            replace=False,\n",
    "            p=data[:, 1] / data[:, 1].sum(),\n",
    "        )\n",
    "        data = data[selected, :]\n",
    "        filenames = [filenames[i] for i in selected]\n",
    "    return [filenames, data]\n",
    "\n",
    "blocks_df[\"clean_patches_relation\"] = blocks_df.patches_relation.apply(\n",
    "    clean_patches_relation\n",
    ")\n",
    "blocks_df[\"n_patches\"] = blocks_df[\"clean_patches_relation\"].apply(\n",
    "    lambda x: x[1].shape[0]\n",
    ")\n",
    "blocks_df = blocks_df[blocks_df.n_patches > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_df = blocks_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique patches: 2214882\n"
     ]
    }
   ],
   "source": [
    "patches_blocks = {}\n",
    "for i, row in blocks_df.iterrows():\n",
    "    relation_list = row[\"patches_relation\"].strip(\" \").split(\" \")\n",
    "    relation_list = [x.split(\";\") for x in relation_list]\n",
    "    relation_list = row[\"clean_patches_relation\"][0]\n",
    "    idx = row[\"clean_patches_relation\"][1][:, 0]\n",
    "    files = [f\"{relation_list[j]} {int(idx[j])}\" for j in range(len(relation_list))]\n",
    "    for file in files:\n",
    "        if file in patches_blocks.keys():\n",
    "            patches_blocks[file].append(i)\n",
    "        else:\n",
    "            patches_blocks[file] = [i]\n",
    "print(f\"Number of unique patches: {len(patches_blocks.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster of patches\n",
    "\n",
    "def get_model(dec = False, k = 10, latent_dim = 128):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = AutoEncoder(\n",
    "        latent_dim = latent_dim,\n",
    "        encoder_arch = \"resnet50_small_patch\",\n",
    "        decoder_layers_per_block = [3] * 5\n",
    "    )\n",
    "    if dec:\n",
    "        model = DEC(\n",
    "            n_clusters = k,\n",
    "            embedding_dim = latent_dim,\n",
    "            encoder = model.encoder\n",
    "        )\n",
    "        model.load_state_dict(torch.load(f\"../models/DEC_resnet50_small_{latent_dim}_k_{k}/model.pt\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(f\"../model/AE_resnet50_small_{latent_dim}/model.pt\"))\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "k = 30\n",
    "latent_dim = 128\n",
    "model = get_model(dec = True, k = k, latent_dim = latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(dl, model):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    clusters = []\n",
    "    clusters_distance = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            c = model(batch).detach().cpu().numpy()\n",
    "            d = model.centroids_distance(batch).detach().cpu().numpy()\n",
    "            clusters.append(c)\n",
    "            clusters_distance.append(d)\n",
    "\n",
    "    clusters = np.concatenate(clusters).argmax(axis = 1)\n",
    "    clusters_distance = np.concatenate(clusters_distance)\n",
    "    return clusters, clusters_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7383/7383 [2:46:05<00:00,  1.35s/it]  \n"
     ]
    }
   ],
   "source": [
    "filenames = list(patches_blocks.keys())\n",
    "dataset = SmallPatchesDataset(filenames)\n",
    "dl = torch.utils.data.DataLoader(dataset, batch_size = 300)\n",
    "clusters, clusters_distance = get_clusters(dl, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to generate features from model output\n",
    "def count_of_patches_cluster():\n",
    "    x = np.zeros((blocks_df.shape[0], k))\n",
    "    for i, (file, cluster) in enumerate(zip(filenames, clusters)):\n",
    "        for b in patches_blocks[file]:\n",
    "            x[b, cluster] += 1\n",
    "    x = pd.DataFrame(x, columns = [f\"cluster_{i}\" for i in range(k)])\n",
    "    x = x.loc[:, x.sum(axis = 0) > 0]\n",
    "    return x\n",
    "\n",
    "def fraction_of_patches_cluster():\n",
    "    x = np.zeros((blocks_df.shape[0], k))\n",
    "    for i, (file, cluster) in enumerate(zip(filenames, clusters)):\n",
    "        for b in patches_blocks[file]:\n",
    "            x[b, cluster] += 1\n",
    "    x_sum = x.sum(axis = 1)\n",
    "    x = x / x_sum[:, None]\n",
    "    x = pd.DataFrame(x, columns = [f\"cluster_{i}\" for i in range(k)])\n",
    "    x = x.loc[:, x.sum(axis = 0) > 0]\n",
    "    x[\"count\"] = x_sum\n",
    "    return x\n",
    "\n",
    "def distance_of_patches_cluster():\n",
    "    x = np.zeros((blocks_df.shape[0], k))\n",
    "    for i, (file, distances) in enumerate(zip(filenames, clusters_distance)):\n",
    "        for b in patches_blocks[file]:\n",
    "            x[b] += distances\n",
    "    x_sum = blocks_df.n_patches.values.reshape(-1)\n",
    "    x = x / x_sum[:, None]\n",
    "    x = pd.DataFrame(x, columns = [f\"cluster_{i}\" for i in range(k)])\n",
    "    x = x.loc[:, x.sum(axis = 0) > 0]\n",
    "    x[\"count\"] = x_sum\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(method):\n",
    "    if method == \"count\":\n",
    "        return count_of_patches_cluster()\n",
    "    elif method == \"fraction\":\n",
    "        return fraction_of_patches_cluster()\n",
    "    elif method == \"distance\":\n",
    "        return distance_of_patches_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(clf, x_train, y_train, x_test, y_test):\n",
    "    mae_train = mean_absolute_error(y_train, clf.predict(x_train))\n",
    "    r2_train = r2_score(y_train, clf.predict(x_train))\n",
    "    \n",
    "    mae_test = mean_absolute_error(y_test, clf.predict(x_test))\n",
    "    r2_test = r2_score(y_test, clf.predict(x_test))\n",
    "    return r2_train, r2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_rf(x_train, y_train, x_test, y_test):\n",
    "    rf = RandomForestRegressor()\n",
    "    parameters = {\n",
    "        \"n_estimators\": [10, 100, 1000],\n",
    "        \"max_depth\": [10, 100],\n",
    "        #\"min_samples_split\": [2, 10, 100],\n",
    "    }\n",
    "    clf = GridSearchCV(rf, parameters, n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return eval(clf, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        for in_dim, out_dim in zip(dims[:-1], dims[1:]):\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if out_dim != dims[-1]:\n",
    "                self.layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.eval()\n",
    "        if type(x) == pd.DataFrame:\n",
    "            x_ = torch.from_numpy(x.values)\n",
    "        elif type(x) == np.ndarray:\n",
    "            x_ = torch.from_numpy(x)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            x_ = x_.to(device)\n",
    "            y = self.layers(x_)\n",
    "            return y.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_mlp(model, dl_train, dl_test):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = []\n",
    "    for i in range(100):\n",
    "        iter_loss = 0\n",
    "        for x, y in dl_train:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_loss += loss.item()\n",
    "\n",
    "        if i % 3 == 0:\n",
    "            iter_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in dl_test:\n",
    "                    x, y = x.to(device), y.to(device)\n",
    "                    y_pred = model(x)\n",
    "                    loss = criterion(y_pred, y)\n",
    "                    iter_loss += loss.item()\n",
    "                test_loss.append(iter_loss)\n",
    "\n",
    "            if i > 10 and test_loss[-1] > test_loss[-2]:\n",
    "                break        \n",
    "\n",
    "def grid_search_mlp(x_train, y_train, x_test, y_test):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    idx_train_, idx_val = train_test_split(np.arange(x_train.shape[0]), test_size = 0.2, random_state = 0)\n",
    "    x_val_, y_val_ = x_train.values[idx_val, :], y_train[idx_val]\n",
    "    x_train_, y_train_ = x_train.values[idx_train_, :], y_train[idx_train_]\n",
    "    x_test_, y_test_ = x_test.values, y_test\n",
    "    scaler = StandardScaler()\n",
    "    x_train_ = scaler.fit_transform(x_train_)\n",
    "    x_val_ = scaler.transform(x_val_)\n",
    "    x_test_ = scaler.transform(x_test_)\n",
    "    dl_train_ = DataLoader(TensorDataset(torch.tensor(x_train_), torch.tensor(y_train_.reshape(-1, 1))), batch_size = 128)\n",
    "    dl_val = DataLoader(TensorDataset(torch.tensor(x_val_), torch.tensor(y_val_).reshape(-1, 1)), batch_size = 128)\n",
    "\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    for dims in [[x_train.shape[1], 32, 64, 32, 1], [x_train.shape[1], 64, 256, 32, 1], [x_train.shape[1], 64, 512, 128, 1]]:\n",
    "        model_1 = MLP(dims)\n",
    "        model_1.to(device, dtype = torch.double)\n",
    "        train_mlp(model_1, dl_train_, dl_val)\n",
    "        r2_train, r2_test = eval(model_1, x_train_, y_train_, x_val_, y_val_)\n",
    "\n",
    "        if r2_test > best_r2:\n",
    "            best_r2 = r2_test\n",
    "            best_model = model_1\n",
    "    \n",
    "    return eval(model_1, x_train_, y_train_, x_test_, y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method: count, target: mhi, regression: mlp, r2_train: 0.041799968292348444, r2_test: 0.03776554291722556\n",
      "method: count, target: mhi, regression: rf, r2_train: 0.13762130340900602, r2_test: 0.03697626731578163\n",
      "method: count, target: ed_attain, regression: mlp, r2_train: 0.0533397636650923, r2_test: 0.035623990381802195\n",
      "method: count, target: ed_attain, regression: rf, r2_train: 0.14967380191264845, r2_test: 0.04107725952430075\n",
      "method: count, target: pop, regression: mlp, r2_train: 0.057335305692281735, r2_test: 0.04856173654560303\n",
      "method: count, target: pop, regression: rf, r2_train: 0.17789558257982452, r2_test: 0.043772433577050296\n",
      "method: fraction, target: mhi, regression: mlp, r2_train: 0.045877053527016876, r2_test: 0.03960599833934242\n",
      "method: fraction, target: mhi, regression: rf, r2_train: 0.14712944318986187, r2_test: 0.035981931920656796\n",
      "method: fraction, target: ed_attain, regression: mlp, r2_train: 0.05951466433903396, r2_test: 0.042960292883652995\n",
      "method: fraction, target: ed_attain, regression: rf, r2_train: 0.15023887954909243, r2_test: 0.04090028965622894\n",
      "method: fraction, target: pop, regression: mlp, r2_train: 0.0514320187788585, r2_test: 0.05068112759569232\n",
      "method: fraction, target: pop, regression: rf, r2_train: 0.15942179054309402, r2_test: 0.05580892364573764\n",
      "method: distance, target: mhi, regression: mlp, r2_train: 0.03304256979232556, r2_test: 0.030804292927856913\n",
      "method: distance, target: mhi, regression: rf, r2_train: 0.14762208590130177, r2_test: 0.03567475505532636\n",
      "method: distance, target: ed_attain, regression: mlp, r2_train: 0.04836413609069967, r2_test: 0.03851963420610427\n",
      "method: distance, target: ed_attain, regression: rf, r2_train: 0.13658705292006035, r2_test: 0.0419113181434303\n",
      "method: distance, target: pop, regression: mlp, r2_train: 0.0431493681639713, r2_test: 0.0467989764825284\n",
      "method: distance, target: pop, regression: rf, r2_train: 0.1447168794768835, r2_test: 0.047542664577216565\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_test = train_test_split(np.arange(blocks_df.shape[0]), test_size = 0.2, random_state = 0)\n",
    "df_results = []\n",
    "for method in [\"count\", \"fraction\", \"distance\"]:\n",
    "    x = get_x(method)\n",
    "    x_train, x_test = x.loc[idx_train, :], x.loc[idx_test, :]\n",
    "    for target in [\"mhi\", \"ed_attain\", \"pop\"]:\n",
    "        y_train, y_test = blocks_df[target].values[idx_train], blocks_df[target].values[idx_test] \n",
    "        \n",
    "        for regression in [\"mlp\", \"rf\"]:\n",
    "            if regression == \"rf\":\n",
    "                r2_train, r2_test = grid_search_rf(x_train, y_train, x_test, y_test)\n",
    "            elif regression == \"mlp\":\n",
    "                r2_train, r2_test = grid_search_mlp(x_train, y_train, x_test, y_test)\n",
    "\n",
    "            print(f\"method: {method}, target: {target}, regression: {regression}, r2_train: {r2_train}, r2_test: {r2_test}\")\n",
    "            df_results.append([method, target, regression, r2_train, r2_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban_gdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
